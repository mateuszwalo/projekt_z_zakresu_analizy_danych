for (case_id in 1:n_cases) {
case_name <- cases[case_id]
for (i in 1:n_iter) {
# generowanie danych
if (grepl("linear_corr_snorm", case_name)) {
# wyciągamy rho z nazwy, np. z 'rho0.5'
rho_val <- as.numeric(sub(".*rho", "", case_name))
df_list <- generate_data(case = "linear_corr_snorm", rho = rho_val)
} else if (grepl("nonlinear_corr", case_name)) {
# wyciągamy a i b z nazwy, np. z 'a1.5b1'
ab_vals <- regmatches(case_name, gregexpr("[0-9\\.]+", case_name))[[1]]
a_val <- as.numeric(ab_vals[1])
b_val <- as.numeric(ab_vals[2])
df_list <- generate_data(case = "nonlinear_corr", a = a_val, b = b_val)
} else {
df_list <- generate_data(case = case_name)
}
df <- df_list$df
# Split 80/20
train_idx <- sample(1:nrow(df), size = 0.8 * nrow(df))
train <- df[train_idx, ]
test <- df[-train_idx, ]
m_simple <- lm(Y ~ X1 + X2, data = train)
m_ideal  <- lm(Y ~ X1 + I(X2^2), data = train)
m_mixed  <- lm(Y ~ X1 + X2 + I(X1 * X2), data = train)
# wyliczanie MSE i zapis współczynników
models <- list(simple = m_simple, ideal = m_ideal, mixed = m_mixed)
for (m_name in names(models)) {
m <- models[[m_name]]
preds <- predict(m, newdata=test)
mse <- mean((test$Y - preds)^2)
coefs <- coef(m)
row <- tibble(
case = case_name,
iter = i,
model = m_name,
mse = mse,
intercept = ifelse("(Intercept)" %in% names(coefs), coefs["(Intercept)"], NA_real_),
b_X1 = ifelse("X1" %in% names(coefs), coefs["X1"], NA_real_),
b_X2 = ifelse("X2" %in% names(coefs), coefs["X2"], NA_real_),
b_X2sq = ifelse("I(X2^2)" %in% names(coefs), coefs["I(X2^2)"], NA_real_),
b_X1X2 = ifelse("I(X1 * X2)" %in% names(coefs), coefs["I(X1 * X2)"], NA_real_)
)
results_list[[idx]] <- row
idx <- idx + 1
}
}
}
results_all <- bind_rows(results_list)
# Zapis wyników
write.csv(results_all, "wyniki_parametry_MSE.csv", row.names = FALSE)
# Statystyki MSE
results_summary <- results_all %>%
group_by(case, model) %>%
summarise(
min_MSE=min(mse),
q25 = quantile(mse, 0.25),
mean_MSE = mean(mse),
median_MSE = median(mse),
q75 = quantile(mse, 0.75),
max_MSE=max(mse),
sd_MSE = sd(mse),
.groups = "drop"
)
write.csv(results_summary, "summary_MSE.csv", row.names = FALSE)
results_summary %>%
mutate(across(where(is.numeric), ~round(., 4))) %>%
kable(caption = "Statystyki MSE dla poszczególnych przypadków i modeli") %>%
kable_styling(full_width = FALSE, bootstrap_options = c("striped", "hover", "condensed")) %>%
column_spec(1, bold = TRUE) %>%
collapse_rows(columns = 1, valign = "middle")
# Wykres: średnie MSE (słupki)
ggplot(results_summary, aes(x = case, y = mean_MSE, fill = model)) +
geom_col(position = "dodge") +
theme_minimal() +
labs(title = "Średnie wartości MSE w poszczególnych przypadkach",
x = "Przypadek",
y = "Średnie MSE") +
theme(axis.text.x = element_text(angle = 45, hjust = 1))
plot_case_fit <- function(case_name) {
df <- read.csv(paste0("data_samples/", case_name, ".csv"))
set.seed(123)
train_idx <- sample(1:nrow(df), size = 0.8 * nrow(df))
train <- df[train_idx, ]
test <- df[-train_idx, ]
m_simple <- lm(Y ~ X1 + X2, data = train)
m_ideal  <- lm(Y ~ X1 + I(X2^2), data = train)
m_mixed  <- lm(Y ~ X1 + X2 + I(X1 * X2), data = train)
test$pred_simple <- predict(m_simple, newdata = test)
test$pred_ideal  <- predict(m_ideal, newdata = test)
test$pred_mixed  <- predict(m_mixed, newdata = test)
mse_simple <- mean((test$Y - test$pred_simple)^2)
mse_ideal  <- mean((test$Y - test$pred_ideal)^2)
mse_mixed  <- mean((test$Y - test$pred_mixed)^2)
var_X1 <- var(test$X1)
var_X2 <- var(test$X2)
var_Y  <- var(test$Y)
summary_table <- tibble(
case = case_name,
var_X1 = var_X1,
var_X2 = var_X2,
var_Y = var_Y,
mse_simple = mse_simple,
mse_ideal = mse_ideal,
mse_mixed = mse_mixed
)
# Wykresy
p1 <- ggplot(test, aes(x = X1, y = Y)) +
geom_point(alpha = 0.5, color = "grey40") +
geom_line(aes(y = pred_simple, color = "Simple"), linewidth = 0.8) +
geom_line(aes(y = pred_ideal, color = "Ideal"), linewidth = 0.8) +
geom_line(aes(y = pred_mixed, color = "Mixed"), linewidth = 0.8) +
labs(
title = paste("Dopasowanie względem X1 -", case_name),
subtitle = sprintf("MSE: Simple=%.3f | Ideal=%.3f | Mixed=%.3f", mse_simple, mse_ideal, mse_mixed),
x = "X1", y = "Y", color = "Model"
) +
theme_minimal()
p2 <- ggplot(test, aes(x = X2, y = Y)) +
geom_point(alpha = 0.5, color = "grey40") +
geom_line(aes(y = pred_simple, color = "Simple"), linewidth = 0.8) +
geom_line(aes(y = pred_ideal, color = "Ideal"), linewidth = 0.8) +
geom_line(aes(y = pred_mixed, color = "Mixed"), linewidth = 0.8) +
labs(
title = paste("Dopasowanie względem X2 -", case_name),
subtitle = sprintf("MSE: Simple=%.3f | Ideal=%.3f | Mixed=%.3f", mse_simple, mse_ideal, mse_mixed),
x = "X2", y = "Y", color = "Model"
) +
theme_minimal()
grid.arrange(p1, p2, ncol = 2)
return(summary_table)
}
plot_case_fit("double_unif_narrow")
plot_case_fit("double_unif_wide")
plot_case_fit("t_norm")
plot_case_fit("t_snorm")
# ---- Wykresy dopasowania modeli (dla jednej ramki z każdego przypadku) ----
for (case_name in cases) {
message("Rysuję dopasowanie dla: ", case_name)
# --- generowanie danych (dokładnie jak przy MSE) ---
if (grepl("linear_corr_snorm", case_name)) {
rho_val <- as.numeric(sub(".*rho", "", case_name))
df_list <- generate_data(case = "linear_corr_snorm", rho = rho_val)
} else if (grepl("nonlinear_corr", case_name)) {
ab_vals <- regmatches(case_name, gregexpr("[0-9\\.]+", case_name))[[1]]
a_val <- as.numeric(ab_vals[1])
b_val <- as.numeric(ab_vals[2])
df_list <- generate_data(case = "nonlinear_corr", a = a_val, b = b_val)
} else {
df_list <- generate_data(case = case_name)
}
df <- df_list$df
# --- podział train/test ---
train_idx <- sample(seq_len(nrow(df)), size = floor(0.8 * nrow(df)))
train <- df[train_idx, ]
test  <- df[-train_idx, ]
# --- dopasowanie modeli (tak samo jak wcześniej) ---
m_simple <- lm(Y ~ X1 + X2, data = train)
m_ideal  <- lm(Y ~ X1 + I(X2^2), data = train)
m_mixed  <- lm(Y ~ X1 + X2 + I(X1 * X2), data = train)
# --- predykcje na zbiorze testowym ---
test$pred_simple <- predict(m_simple, newdata = test)
test$pred_ideal  <- predict(m_ideal,  newdata = test)
test$pred_mixed  <- predict(m_mixed,  newdata = test)
# --- wykres dopasowania względem X2 ---
p <- ggplot(test, aes(x = X2, y = Y)) +
geom_point(alpha = 0.5, color = "gray40") +
geom_line(aes(y = pred_simple, color = "Model 1: X1+X2"), linewidth = 0.8) +
geom_line(aes(y = pred_ideal,  color = "Model 2: X1+X2²"), linewidth = 0.8) +
geom_line(aes(y = pred_mixed,  color = "Model 3: X1+X2+X1·X2"), linewidth = 0.8) +
scale_color_manual(values = c("firebrick3", "steelblue3", "darkgreen")) +
labs(
title = paste("Dopasowanie modeli dla przypadku:", case_name),
subtitle = "Dane testowe (20%) + trzy dopasowania",
x = "X2",
y = "Y",
color = "Model"
) +
theme_minimal()
print(p)
}
# Diagnostyka modelu (dla jednej ramki z każdego przypadku)
for (case_name in cases) {
# generowanie danych z odpowiednimi parametrami
if (grepl("linear_corr_snorm", case_name)) {
# wyciągamy rho z nazwy, np. z 'rho0.5'
rho_val <- as.numeric(sub(".*rho", "", case_name))
df_list <- generate_data(case = "linear_corr_snorm", rho = rho_val)
} else if (grepl("nonlinear_corr", case_name)) {
# wyciągamy a i b z nazwy, np. z 'a1.5b1'
ab_vals <- regmatches(case_name, gregexpr("[0-9\\.]+", case_name))[[1]]
a_val <- as.numeric(ab_vals[1])
b_val <- as.numeric(ab_vals[2])
df_list <- generate_data(case = "nonlinear_corr", a = a_val, b = b_val)
} else {
df_list <- generate_data(case = case_name)
}
df <- df_list$df
# dopasowanie modelu
model <- lm(Y ~ X1 + I(X2^2), data = df)
# wykresy diagnostyczne
par(mfrow = c(2, 2))
plot(model, main = paste("Diagnostyka modelu:", case_name))
}
library(dplyr)
library(tibble)
library(purrr)
library(ggplot2)
library(knitr)
library(kableExtra)
library(scales)
dfs <- list(
df1 = df1_hidden_impact,
df2 = df2_hidden_impact,
df3 = df3_hidden_impact
)
pc1_score <- function(data){ pr <- prcomp(scale(dplyr::select(data, X1, X2)))
list(score = as.numeric(pr$x[,1]), var_expl = pr$sdev[1]^2 / sum(pr$sdev^2)) }
proxy_tbl <- imap_dfr(dfs, function(dfi, nm){
pc1 <- pc1_score(dfi)
tibble(
zbior = nm,
cor_X1_X2 = cor(dfi$X1, dfi$X2),
var_expl_PC1 = round(pc1$var_expl, 3),
cor_PC1_X1 = cor(pc1$score, dfi$X1),
cor_PC1_X2 = cor(pc1$score, dfi$X2)
)
})
proxy_tbl %>%
transmute(
`Zbiór`                = zbior,
`Corr(X1, X2)`         = round(cor_X1_X2, 3),
`Corr(PC1, X1)`        = round(cor_PC1_X1, 3),
`Corr(PC1, X2)`        = round(cor_PC1_X2, 3),
`PC1: udział wariancji`= scales::percent(var_expl_PC1, accuracy = 0.1)
) %>%
kable(align = "lcccc",
booktabs = TRUE,
caption = "Sygnały wspólnej przyczyny (proxy-Z)") %>%
kable_styling(full_width = FALSE,
bootstrap_options = c("striped", "hover")) %>%
column_spec(1, bold = TRUE) %>%
add_header_above(c(" " = 1, "Korelacje" = 3, " " = 1))
pc1_all <- purrr::imap_dfr(dfs, function(dfi, nm){
pc <- pc1_score(dfi)
tibble(
zbior    = nm,
PC1      = pc$score,
var_expl = pc$var_expl
)
})
facet_labels <- pc1_all %>%
distinct(zbior, var_expl) %>%
mutate(label = paste0(zbior, " — PC1: ", percent(var_expl, accuracy = 0.1))) %>%
{ setNames(.$label, .$zbior) }
pc1_hist <- ggplot(pc1_all, aes(x = PC1)) +
geom_histogram(aes(y = ..density..), bins = 30, alpha = 0.75) +
geom_density(adjust = 1.2, linewidth = 0.8) +
geom_vline(xintercept = 0, linetype = "dashed") +
facet_wrap(~ zbior, scales = "free_x", labeller = as_labeller(facet_labels)) +
labs(
title    = "Rozkład PC1 (proxy-Z) w zestawach hidden_impact",
x        = "PC1 (standaryzowany score z PCA na {X1, X2})",
y        = "Gęstość"
) +
theme_minimal(base_size = 12) +
theme(
strip.text = element_text(face = "bold"),
plot.title = element_text(face = "bold")
)
pc1_hist
form_simple <- Y ~ X1 + X2
form_ideal <- Y ~ X1 + I(X2^2)
cv_mse <- function(form, data, K = 5, seed = 123){
set.seed(seed)
n <- nrow(data)
folds <- sample(rep(1:K, length.out = n))  # losowe przypisanie do K foldów
mse_folds <- sapply(1:K, function(k){
tr <- data[folds != k, , drop = FALSE]
te <- data[folds == k, , drop = FALSE]
fit <- lm(form, data = tr)
mean((te$Y - predict(fit, newdata = te))^2)
})
list(mean = mean(mse_folds),
sd   = sd(mse_folds),
se   = sd(mse_folds) / sqrt(K),
per_fold = mse_folds)
}
K <- 5
seed_cv <- 123
cv_tbl <- imap_dfr(dfs, function(dfi, nm){
cv_s <- cv_mse(form_simple, dfi, K = K, seed = seed_cv)
cv_i <- cv_mse(form_ideal,  dfi, K = K, seed = seed_cv)
tibble(
zbior  = nm,
model  = c("simple", "ideal"),
CV_MSE = c(cv_s$mean, cv_i$mean),
SE     = c(cv_s$se,   cv_i$se)
)
})
cv_folds_long <- imap_dfr(dfs, function(dfi, nm){
list(
tibble(zbior = nm, model = "simple", fold = 1:K,
mse = cv_mse(form_simple, dfi, K = K, seed = seed_cv)$per_fold),
tibble(zbior = nm, model = "ideal",  fold = 1:K,
mse = cv_mse(form_ideal,  dfi, K = K, seed = seed_cv)$per_fold)
) |> bind_rows()
})
library(dplyr)
library(tidyr)
library(knitr)
library(kableExtra)
library(scales)
table_pretty <- cv_tbl %>%
dplyr::select(zbior, model, CV_MSE, SE) %>%
dplyr::mutate(CI_low = CV_MSE - 1.96 * SE,
CI_hi  = CV_MSE + 1.96 * SE) %>%
tidyr::pivot_wider(names_from = model, values_from = c(CV_MSE, SE, CI_low, CI_hi)) %>%
dplyr::mutate(
`Δ (simple - ideal)`           = CV_MSE_simple - CV_MSE_ideal,
`Rel. poprawa ideal vs simple` = ifelse(
is.finite(CV_MSE_simple) & CV_MSE_simple != 0,
(CV_MSE_simple - CV_MSE_ideal) / CV_MSE_simple, NA_real_
)
)
table_pretty %>%
dplyr::mutate(
dplyr::across(
.cols = c(CV_MSE_simple, SE_simple, CI_low_simple, CI_hi_simple,
CV_MSE_ideal,  SE_ideal,  CI_low_ideal,  CI_hi_ideal,
`Δ (simple - ideal)`),
~ round(.x, 4)
),
`Rel. poprawa ideal vs simple` = percent(`Rel. poprawa ideal vs simple`, accuracy = 0.1)
) %>%
dplyr::rename(
Zbiór       = zbior,
`CV-MSE`    = CV_MSE_simple, `SE`    = SE_simple,
`CI (low)`  = CI_low_simple, `CI (hi)` = CI_hi_simple,
`CV-MSE `   = CV_MSE_ideal,  `SE `   = SE_ideal,
`CI (low) ` = CI_low_ideal,  `CI (hi) ` = CI_hi_ideal
) %>%
knitr::kable(align = "lrrrrrrrrrr", booktabs = TRUE,
caption = "K-fold CV-MSE (K=5): porównanie modeli bez Z (średnia, SE, 95% CI, różnice)") %>%
kableExtra::kable_styling(full_width = FALSE, bootstrap_options = c("striped","hover")) %>%
kableExtra::add_header_above(c(" " = 1, "Model: simple" = 4, "Model: ideal" = 4, "Różnice" = 2)) %>%
kableExtra::column_spec(1, bold = TRUE)
#| echo: false
p_cv_bar <- ggplot(cv_tbl, aes(x = model, y = CV_MSE, fill = model)) +
geom_col(width = 0.65) +
geom_errorbar(aes(ymin = CV_MSE - SE, ymax = CV_MSE + SE),
width = 0.15, linewidth = 0.6) +
facet_wrap(~ zbior, scales = "free_y") +
labs(
title = "CV-MSE (K=5) z błędami standardowymi",
subtitle = "Porównanie generalizacji: model prosty vs idealny (bez Z)",
x = "Model",
y = "CV-MSE"
) +
theme_minimal(base_size = 12) +
theme(legend.position = "none",
strip.text = element_text(face = "bold"),
plot.title = element_text(face = "bold"))
p_cv_bar
plot_pred_vs_actual <- function(case_name) {
df <- read.csv(paste0("data_samples/", case_name, ".csv"))
set.seed(123)
train_idx <- sample(1:nrow(df), size = 0.8 * nrow(df))
train <- df[train_idx, ]
test <- df[-train_idx, ]
# m_simple <- lm(Y ~ X1 + X2, data = train)
# m_ideal  <- lm(Y ~ X1 + I(X2^2), data = train)
# m_mixed  <- lm(Y ~ X1 + X2 + I(X1 * X2), data = train)
# Predykcje
test$pred_simple <- predict(m_simple, newdata = test)
test$pred_ideal  <- predict(m_ideal, newdata = test)
test$pred_mixed  <- predict(m_mixed, newdata = test)
# Obliczanie MSE
mse_simple <- mean((test$Y - test$pred_simple)^2)
mse_ideal  <- mean((test$Y - test$pred_ideal)^2)
mse_mixed  <- mean((test$Y - test$pred_mixed)^2)
# Pomocnicza funkcja do tworzenia wykresu dla jednego modelu
make_plot <- function(pred_col, model_name, mse_value, color) {
ggplot(test, aes(x = Y, y = !!sym(pred_col))) +
geom_abline(intercept = 0, slope = 1, linetype = "dashed", color = "black") +
geom_segment(aes(xend = Y, yend = !!sym(pred_col)), alpha = 0.4, color = color) +
geom_point(color = color, size = 1.5) +
labs(
title = paste("Predicted vs Actual -", model_name),
subtitle = sprintf("%s | MSE = %.4f", case_name, mse_value),
x = "Actual Y",
y = "Predicted Ŷ"
) +
theme_minimal() +
coord_equal()
}
# Trzy wykresy obok siebie
p1 <- make_plot("pred_simple", "Simple", mse_simple, "darkred")
p2 <- make_plot("pred_ideal", "Ideal", mse_ideal, "steelblue")
p3 <- make_plot("pred_mixed", "Mixed", mse_mixed, "darkgreen")
grid.arrange(p1, p2, p3, ncol = 3)
}
plot_pred_vs_actual(double_unif_wide)
plot_pred_vs_actual("double_unif_wide")
plot_pred_vs_actual <- function(case_name) {
df <- read.csv(paste0("data_samples/", case_name, ".csv"))
set.seed(123)
train_idx <- sample(1:nrow(df), size = 0.8 * nrow(df))
train <- df[train_idx, ]
test <- df[-train_idx, ]
m_simple <- lm(Y ~ X1 + X2, data = train)
m_ideal  <- lm(Y ~ X1 + I(X2^2), data = train)
m_mixed  <- lm(Y ~ X1 + X2 + I(X1 * X2), data = train)
# Predykcje
test$pred_simple <- predict(m_simple, newdata = test)
test$pred_ideal  <- predict(m_ideal, newdata = test)
test$pred_mixed  <- predict(m_mixed, newdata = test)
# Obliczanie MSE
mse_simple <- mean((test$Y - test$pred_simple)^2)
mse_ideal  <- mean((test$Y - test$pred_ideal)^2)
mse_mixed  <- mean((test$Y - test$pred_mixed)^2)
# Pomocnicza funkcja do tworzenia wykresu dla jednego modelu
make_plot <- function(pred_col, model_name, mse_value, color) {
ggplot(test, aes(x = Y, y = !!sym(pred_col))) +
geom_abline(intercept = 0, slope = 1, linetype = "dashed", color = "black") +
geom_segment(aes(xend = Y, yend = !!sym(pred_col)), alpha = 0.4, color = color) +
geom_point(color = color, size = 1.5) +
labs(
title = paste("Predicted vs Actual -", model_name),
subtitle = sprintf("%s | MSE = %.4f", case_name, mse_value),
x = "Actual Y",
y = "Predicted Ŷ"
) +
theme_minimal() +
coord_equal()
}
# Trzy wykresy obok siebie
p1 <- make_plot("pred_simple", "Simple", mse_simple, "darkred")
p2 <- make_plot("pred_ideal", "Ideal", mse_ideal, "steelblue")
p3 <- make_plot("pred_mixed", "Mixed", mse_mixed, "darkgreen")
grid.arrange(p1, p2, p3, ncol = 3)
}
plot_pred_vs_actual("double_unif_wide")
plot_pred_vs_actual <- function(case_name) {
df <- read.csv(paste0("data_samples/", case_name, ".csv"))
set.seed(123)
train_idx <- sample(1:nrow(df), size = 0.8 * nrow(df))
train <- df[train_idx, ]
test <- df[-train_idx, ]
m_simple <- lm(Y ~ X1 + X2, data = train)
m_ideal  <- lm(Y ~ X1 + I(X2^2), data = train)
m_mixed  <- lm(Y ~ X1 + X2 + I(X1 * X2), data = train)
# Predykcje
test$pred_simple <- predict(m_simple, newdata = test)
test$pred_ideal  <- predict(m_ideal, newdata = test)
test$pred_mixed  <- predict(m_mixed, newdata = test)
# Obliczanie MSE
mse_simple <- mean((test$Y - test$pred_simple)^2)
mse_ideal  <- mean((test$Y - test$pred_ideal)^2)
mse_mixed  <- mean((test$Y - test$pred_mixed)^2)
# Pomocnicza funkcja do tworzenia wykresu dla jednego modelu
make_plot <- function(pred_col, model_name, mse_value, color) {
ggplot(test, aes(x = Y, y = !!sym(pred_col))) +
geom_abline(intercept = 0, slope = 1, linetype = "dashed", color = "black") +
geom_segment(aes(xend = Y, yend = !!sym(pred_col)), alpha = 0.4, color = color) +
geom_point(color = color, size = 1.5) +
labs(
title = paste("Przewidziana vs Prawdziwa wartość -", model_name),
subtitle = sprintf("%s | MSE = %.4f", case_name, mse_value),
x = "Prawdziwy Y",
y = "Przewidziany $\hat{Y}$"
plot_pred_vs_actual <- function(case_name) {
df <- read.csv(paste0("data_samples/", case_name, ".csv"))
set.seed(123)
train_idx <- sample(1:nrow(df), size = 0.8 * nrow(df))
train <- df[train_idx, ]
test <- df[-train_idx, ]
m_simple <- lm(Y ~ X1 + X2, data = train)
m_ideal  <- lm(Y ~ X1 + I(X2^2), data = train)
m_mixed  <- lm(Y ~ X1 + X2 + I(X1 * X2), data = train)
# Predykcje
test$pred_simple <- predict(m_simple, newdata = test)
test$pred_ideal  <- predict(m_ideal, newdata = test)
test$pred_mixed  <- predict(m_mixed, newdata = test)
# Obliczanie MSE
mse_simple <- mean((test$Y - test$pred_simple)^2)
mse_ideal  <- mean((test$Y - test$pred_ideal)^2)
mse_mixed  <- mean((test$Y - test$pred_mixed)^2)
# Pomocnicza funkcja do tworzenia wykresu dla jednego modelu
make_plot <- function(pred_col, model_name, mse_value, color) {
ggplot(test, aes(x = Y, y = !!sym(pred_col))) +
geom_abline(intercept = 0, slope = 1, linetype = "dashed", color = "black") +
geom_segment(aes(xend = Y, yend = !!sym(pred_col)), alpha = 0.4, color = color) +
geom_point(color = color, size = 1.5) +
labs(
title = paste("Przewidziana vs Prawdziwa wartość -", model_name),
subtitle = sprintf("MSE = %.4f", mse_value),
x = "Prawdziwy Y",
y = "Przewidziany Ŷ"
) +
theme_minimal() +
coord_equal()
}
# Trzy wykresy obok siebie
p1 <- make_plot("pred_simple", "Simple", mse_simple, "darkred")
p2 <- make_plot("pred_ideal", "Ideal", mse_ideal, "steelblue")
p3 <- make_plot("pred_mixed", "Mixed", mse_mixed, "darkgreen")
grid.arrange(p1, p2, p3, ncol = 3)
}
plot_pred_vs_actual("double_unif_wide")
plot_pred_vs_actual("double_unif_wide")
plot_pred_vs_actual("double_unif_narrow")
